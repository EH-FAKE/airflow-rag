# Base configuration for Airflow services
x-airflow-common: &airflow-common
  build: .
  user: "${AIRFLOW_UID:-50000}:0"
  env_file:
    - .env
  volumes:
    - ./airflow_ragflow/airflow.cfg:${AIRFLOW_HOME}/airflow.cfg
    - ./airflow_ragflow/dags:${AIRFLOW_HOME}/dags/
    - ./airflow_ragflow/plugins:${AIRFLOW_HOME}/plugins/
    - ./airflow_ragflow/helpers:${AIRFLOW_HOME}/helpers/
    - ./airflow_ragflow/spark_jobs:${AIRFLOW_HOME}/spark_jobs/
  depends_on: &airflow-common-depends-on
    postgres:
      condition: service_healthy

# Common environment variables for Airflow
x-airflow-environment: &airflow-common-env
  AIRFLOW__CORE__DEFAULT_TIMEZONE: 'America/Sao_Paulo'
  AIRFLOW__CORE__ENABLE_XCOM_PICKLING: 'true'
  AIRFLOW__CORE__EXECUTOR: LocalExecutor
  AIRFLOW__CORE__FERNET_KEY: ${AIRFLOW__CORE__FERNET_KEY}
  AIRFLOW__CORE__LOAD_EXAMPLES: 'false'
  AIRFLOW__CORE__TEST_CONNECTION: Enabled
  AIRFLOW__API__AUTH_BACKENDS: airflow.api.auth.backend.basic_auth
  AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://${POSTGRES_USER:-airflow}:${POSTGRES_PASSWORD:-airflow}@postgres/airflow
  AIRFLOW__EMAIL__DEFAULT_EMAIL_ON_RETRY: 'false'
  AIRFLOW__EMAIL__DEFAULT_EMAIL_ON_FAILURE: 'false'
  AIRFLOW__WEBSERVER__DEFAULT_UI_TIMEZONE: 'America/Sao_Paulo'
  AIRFLOW__WEBSERVER__INSTANCE_NAME: "RAGFlow - Local Dev!"
  AIRFLOW__WEBSERVER__NAVBAR_COLOR: '#CCA9DD'
  AIRFLOW__WEBSERVER__RELOAD_ON_PLUGIN_CHANGE: 'true'
  AIRFLOW__WEBSERVER__SECRET_KEY: '42'
  AIRFLOW__CORE__LOAD_DEFAULT_CONNECTIONS: 'false'
  PYTHONPATH: "/opt/airflow/spark_jobs:/opt/airflow/dags:/opt/airflow/plugins:/opt/airflow/helpers:$PYTHONPATH"
  _AIRFLOW_DB_MIGRATE: 'true'
  _AIRFLOW_WWW_USER_CREATE: 'true'
  _AIRFLOW_WWW_USER_USERNAME: ${_AIRFLOW_WWW_USER_USERNAME:-airflow}
  _AIRFLOW_WWW_USER_PASSWORD: ${_AIRFLOW_WWW_USER_PASSWORD:-airflow}
  AIRFLOW__CORE__PLUGINS_FOLDER: ${AIRFLOW_HOME}/plugins
  AIRFLOW__CORE__DAGS_FOLDER: ${AIRFLOW_HOME}/dags
  AIRFLOW_REPO_BASE: ${AIRFLOW_HOME}
  # A ENV AIRFLOW_REPO_BASE É IMPORTANTE PARA SINCRONIZAR COM O SISTEMA DE PASTAS
  # DO AIRFLOW EM HOMOLOG E PROD, ELES POSSUEM UMA ESTRUTURA DE PASTAS DIFERENTE
  # USAR ESSA ENV NOS CÓDIGOS PARA NÃO HAVER CONFLITOS
  
services:
  # Airflow Services
  airflow:
    <<: *airflow-common
    # Usamos o comando de standalone aqui para não rodar múltiplos containers
    # do airflow localmente e assim melhorar a velocidade inicialização
    # doc do standalone: https://airflow.apache.org/docs/apache-airflow/2.8.1/start.html
    command: standalone
    ports:
      - "8080:8080"
    healthcheck:
      test: [ "CMD", "curl", "--fail", "http://localhost:8080/health" ]
      interval: 10s
      timeout: 60s
      start_period: 60s
      retries: 5
    restart: always
    environment:
      <<: *airflow-common-env

  # Postgres database
  postgres:
    image: postgres:15-alpine
    env_file:
      - .env
    environment:
      POSTGRES_DB: ${POSTGRES_DB}
      POSTGRES_USER: ${POSTGRES_USER}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
    volumes:
      - postgres-db:/var/lib/postgresql/data
      # - ./docker/postgres/:/docker-entrypoint-initdb.d/
    ports:
      - "5432:5432"
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U $POSTGRES_USER"]
      interval: 10s
      timeout: 5s
      retries: 5
    restart: always

  minio:
    image: minio/minio:RELEASE.2025-01-20T14-49-07Z-cpuv1
    env_file:
      - .env
    ports:
      - "9000:9000"
      - "9001:9001"
    volumes:
      - ./mnt/minio:/data
    command: server --console-address ":9001" /data

  spark:
    image: bitnami/spark:3.5
    env_file:
      - .env
    container_name: spark
    environment:
      - SPARK_MODE=master
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
    ports:
      - "7077:7077"     # Porta RPC (spark://)
      - "8081:8081"     # Web UI do Spark master
    networks:
      - default

  spark-worker-1:
    image: bitnami/spark:3.5
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark:7077
    depends_on:
      - spark


volumes:
  postgres-db:
  airflow_logs:
